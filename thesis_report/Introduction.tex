%!TEX root = tesi.tex
\chapter{Introduction}
\label{ch:introduction}

Data-intensive computing requires seamless processing power which is often not available at the network-edge but rather hosted in the cloud platforms. The huge amount of mobile and IoT devices that has become available in the past few years, is able to produce a massive quantity of data, introducing several (big) data challenges and opportunities. The majority of these devices do not have or can not handle the computational requirements to process the data they capture, outsourcing to the cloud the responsibility to perform (some or all) computations. This process of transferring computation tasks to another platform is called {\it computation offloading} and it is crucial to the mobile devices because it results in lower processing time and energy consumption~\footnote{Despite being a less popular practice, the process of offloading can also be run ``backwards``, that is, tasks may be offloaded from the cloud to other (mobile) devices to lighten an overwhelmed cloud server.}.  In critical scenarios, such as natural or man-made disasters, where the physical network infrastructure may be highly unreliable or even unavailable, not only computation offloading becomes necessary, but latency requirements become more strict, making path management solutions, such as (mobile-generated) traffic steering, essential to satisfy application requirements. Current traffic steering or offloading solutions are usually performance-unaware (e.g., OSPF, ECMP), achieving therefore sub-optimal performance. 

Data driven networking~\cite{jiang2017unleashing} is a recently adopted paradigm that has been used to fill the performance-unaware gap of many network decision problems. The idea behind Data-driven networking~\footnote{An alternative term used for Data driven networking has been knowledge-defined networking~\cite{clark2003knowledge,mestres2017knowledge}} is to wonder what data can do for networking, so that the network's control plane can be rethought and redesigned to overcome current limitations. These limitations, caused by \textit{manually designed} control strategies are becoming more and more ineffective because of  growing expectations of user-perceived Quality of Experience~\cite{jiang2014eona}, a bigger decision space for control decisions and an increasing number of application operating conditions.
Many researchers have proposed the use of machine learning techniques to solve networking problems, including traffic classification~\cite{nguyen2008survey}, latency prediction~\cite{end-to-end} and video streaming bitrate optimization~\cite{mao2017neural}; to our knowledge, this is the first attempt to use Long-Short Term Memory (LSTM)~\cite{hochreiter1997long} to train network devices to steer traffic in a virtual network. 

Routing is one of the most fundamental networking mechanism and, consequently, has been widely researched to be optimized in a variety of context, such as ISP networks and data centers. Usually route-optimization processes produce configurations based on previously observed traffic conditions, or configurations optimized for a range of feasible traffic scenarios, hoping to cover the entire range. The purpose of using machine learning is to leverage past traffic information to learn good routing configurations for future conditions. It is reasonable to assume that the history of traffic contains information about the future, for example, the traffic distribution at different times of the day. The idea behind data driven networking is therefore to observe traffic demands and adapt future routing decisions accordingly. In this work we develop an architecture to be deployed at the edge of the network to assist the offloading process and make use of machine learning techniques to perform path management. Our hypothesis in this work has been to evaluate whether or not classic (node or link) offloading policies can be outperformed in terms of latency and throughput by learning implicit patterns in network traces.

In particular, in this thesis we make the following contributions:
\begin{itemize}
\item we design of an edge computing architecture for (node and link) \textbf{offloading} management within edge computing, that is, identifying the mechanisms (i.e. macro blocks) required for such a system
\item we propose a simple yet effective protocol for mobile computation (task) offloading 
\item we propose a deep learning based \textbf{path prediction system} as part of the offloading architecture; this system is meant to be used as an offloading policy in the proposed architecture 
\item we evaluate the performance of the proposed  approach using different use cases: we  first evaluate the prediction system on its ability to learn from an existing model, and then as a routing algorithm (replacing state of the art solutions such as OSPF and ECMP).
\end{itemize}

The rest of this thesis is organized as follows:
\paragraph{Chapter~\ref{ch:introduction}} explains the motivation and the	purpose of this work.
\paragraph{Chapter~\ref{ch:related_work}} illustrates a summary of the related work. 
\paragraph{Chapter~\ref{ch:background}} contains a brief background about machine learning and networking notions and overviews the main techniques used in this project.
\paragraph{Chapter~\ref{ch:contribution}} describes in details the architecture and the implementation of the path predictor system.
\paragraph{Chapter~\ref{ch:results}} shows considerations and results of the implemented system.
\paragraph{Chapter~\ref{ch:conclusion}} presents comments about the outcome of the project and possible future developments.
\endinput