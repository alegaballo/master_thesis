\chapter{Offloading Architecture}
\label{ch:contribution}
This chapter is organized as follows: section~\ref{sec:task_off} illustrates the task offloading architecture and its implementation, section~\ref{sec:path_prediction} describes the path prediction system and its details and finally, section~\ref{sec:implementation}, summarizes the tools adopted in the development of the project.

\section{Task Offloading}
\label{sec:task_off}
Edge computing is a recent computing paradigm whose main idea is to push (i.e. delegate) data processing at the edge of the network for fast pre-processing within latency-sensitive applications. The goal is to reduce communication costs by keeping computations close to the source of data. To the best of our knowledge, there has not been any effort in trying to implement a general abstraction that provides the guidelines for the protocol implementations. The proposed system is a first draft of how such abstraction could be implemented, with all the limitations of a six-months work.
\begin{figure}[]
\centering
\includegraphics[width=0.75\textwidth]{img/off_sys_arch}
\caption{Offloading system architecture.}
\label{fig:offloading_system_arch}
\end{figure}


\subsection{Architecture}
The system architecture is described in figure~\ref{fig:offloading_system_arch}; we consider a scenario in which mobile devices want to offload tasks to the edge cloud in a modern network that supports SDN.

The main components are:
\begin{itemize}
\item mobile device interface: interface for communications between the mobile devices and the offloading system
\item edge cloud interface: interface for communications between the edge cloud and the offloading system
\item offloading logic: offloading policy to be used to serve the edge client
\item SDN controller: responsible of enforcing the offloading policies on the network devices.
\end{itemize}

The mobile device interface supports the communication between the mobile devices and the offloading system, it provides a set of primitives necessary to the two parties to communicate efficiently and formalizes the offloading request requirements. The edge cloud interface has the same role of the mobile device interface, but the involved parties are different: in this case the primitives are meant for the communication between the offloading system and the edge cloud, but the objective remains the same. The offloading logic is the main part of the architecture, it contains the set of policies available as offloading criterion, allowing the mobile devices to specify which one they intend to use and users to implement their own. Finally, the architecture includes a SDN controller, that we believe, must be part of it, because of the numerous possibilities that SDN offers in terms of network management.  

\subsection{Offloading Protocol}
Through the edge and cloud interfaces, the parties communicate in order to complete a task offloading process. For this communication to happen, a protocol is required. In this context the parties are:
\begin{itemize}
    \item mobile device: sends request to the offloading server
    \item offloading server: accepts request from the mobile device, enforces the offloading policy by talking to the SDN controller and forwards the tasks to the edge server
    \item SDN controller: receives flows installation requests from the offloading server
    \item edge server: receives the tasks that need to be offloaded.
\end{itemize}

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{img/protocol_sequence}   
\caption{Offloading workflow: mobile devices and an offloading server orchestrate the request via an SDN controller}
\label{fig:protocol}
\end{figure}

Figure~\ref{fig:protocol} shows the typical message sequence needed to complete an offloading request; the required steps are the following:
\begin{enumerate}
\item the mobile device sends an offload request to the offloading server
\item the offloading server decides whether it is possible or not to accept the request and if so, where to offload it according to the specified or default policy
\item the offloading server asks the SDN controller to install on the switches the flows required by the offloading policy
\item the offloading server forwards the offloaded task to the edge server 
\item the edge server sends the computation result back to the offloading server forwards it to the device.
\end{enumerate}

Task offloading is a complex problem, with a high number of factors involved in the final decision. These factors include application requirements formalization, task retrieval, edge server/cloud discovery. Before proceeding with the protocol implementation, we address some of these problems.\\\\
\textit{Application requirements formalization}

As a starting point we assume that the application requirements are expressed in terms of \textbf{cpu ratio}(average cpu used to execute the task on the mobile device), \textbf{memory footprint} (quantity of memory required by the application) and desired \textbf{latency} (specifying if the task, is urgent or it can wait).\\\\
\textit{Task retrieval}

For the task retrieval problem we consider two different scenarios: in the first one the task is hosted on the server with the possibility to retrieve it with a unique identifier; in the second scenario, the task is sent to the server, wrapped in a container (e.g JAR, EGG).\\\\
\textit{Edge servers discovery}

Since the implementation of a server discovering protocol is out of the scope of this project, we assume that the offloading server is aware of the available edge servers.\\\\
With these simplifications in mind, we carry out the protocol messages definition using Google Protocol Buffer~\cite{protobuf}, a library that allows to easily specify structured messages through a \textit{.proto} file. The complete .proto file is available in the Appendix~\ref{proto_file} at the end of the report; what follows is a brief overview of the protocol main messages.

\paragraph{OffloadRequest} is the message sent from the mobile device to the offloading server and includes the requirements, the type of the task and optionally the task itself.

The \textbf{requirements} consist of the specification of the amount of cpu and memory necessary for the task to be executed and the level of latency needed. The idea of having a latency level allows a better support for real-time applications.

The \textbf{type} of the task is needed to support server-less computing~\cite{hendrickson2016serverless}~\cite{fox2017status} and implies two possible scenarios. In case of server-less computing the type of the task is set to \textit{LAMBDA} and it is assumed that the task is already on the server. If the type is set to \textit{STANDARD} then the code to be executed is sent as payload of the request.

The \textbf{task} could be an identifier (\textit{task\_id}) of the task on the edge server (in case of a lambda application), or the task itself. In the latter case, the executable and its type are included in the request in a message called \textit{TaskWrapper}.

\paragraph{Response} is a message used for several purposes; it is used to confirm the reception of a message or to signal an error. Response is also used to send back to the mobile device the result of the computation. It is important to notice that protobuf messages are not self-delimiting, that means that it is necessary to know its size in order to receive it. Every time a new message needs to be sent, the sender starts with the message size first, if an OK response is received then the message is sent.

\paragraph{Message} is a wrapper for all the messages used in the system, its purpose is to ease the parsing process by having a \textit{type} field that can be one of the messages defined above.


\section{Path prediction via Deep Learning}
\label{sec:path_prediction}
The architecture described in the previous section includes the \textit{offloading logic} block, responsible of determining the criterion on which the offloading is based. As a first offloading policy, we implement a deep learning model capable of finding the path towards a destination, that should be used in place of traditional routing policies. 

One of the problem of traditional routing algorithms is that they do not take into account how the network load changes over time: the path from a source to a destination is computed by taking into account static parameters, such as the nominal interface speed. This lack of consideration of the dynamic behavior of the network can cause traffic slowdown and packet loss in case of congestion, an undesired behavior in circumstances where latency is crucial. The intuition behind our project is that \textit{collaborative traffic steering} should be able to identify and avoid congestion situations. A collaborative policy requires in some way the participation to all the parts in the decision process, however, achieving nodes collaboration in a network is a complicated task. A simplified version of this collaborative process could be the following: instead of all the nodes to be directly involved in the decision process, they could limit their role to notify a central node of their current status. This is made possible by the SDN paradigm: in our system the nodes are connected to a SDN controller responsible of retrieving information about the nodes status, in this case using the OpenFlow protocol. %openflow architecture picture 
The information used in our system is the number of incoming packets on a node: the idea is that the packet distribution on the nodes, routers in this case, reflects the network conditions; a high packet count on a router is an indicator of a big load that is probably going to lead to packet loss and retransmission. Another point to clarify before proceeding with the implementation details, is that the distribution of packets on the routers is influenced by the routing algorithms: nodes that appear in multiple paths will probably have an higher count than less traversed nodes because they are responsible of forwarding packets for multiple source-destination pairs. If we were able to see all the possible outcome of a routing protocol in a network and extract the consequent traffic patterns, we could try to choose the less busy path. This is exactly how we build our deep learning model: we simulate a small network with ten routers, we choose a routing algorithm (OSPF), we make it vary and record the traffic patterns. Afterwards, we use the collected data and the routing choices taken by the routing protocol to build a model capable of predicting each hop of the path, from the source to the destination.

To create a working deep learning model, two elements are essentials: the dataset and the network architecture; the following sections describe the type of data, how we obtain them and what neural network architecture we use. 

\subsection{Dataset}
The performance of a machine learning model depends heavily on the data used to train it. Creating a working model requires having good data in terms of quantity and quality, namely the number of samples available to train the model and how well these samples represent the domain you are trying to model. Training a model with a dataset too small usually results in poor performance given that  the model does not have enough information to learn; on the other hand, even a big dataset which covers only a small portion of the domain of study, will perform poorly because the system, will not be able to learn all of the different scenarios, therefore losing the ability to generalize and creating an abstraction.

Recently a lot of effort is being put in making datasets available to the machine learning community, with the aim of promoting research. Regarding networks, there is plenty of dataset available ~\cite{caida}\cite{fb_dataset}\cite{topo_zoo}; however those are mostly network captures of datacenters or small portions of networks, without any detail about the underlying topology nor the routing strategies. To implement our path prediction system, given a network, we need:
\begin{itemize}
\item the network topology
\item the routing informations
\item the packet count on each node.	
\end{itemize}

\subsubsection{Topology}
\label{sec:net_topology}
The first component of our system is the network: we need to emulate a network from which we can extract the data necessary to train the model and to use to test the final model. Mininet~\cite{mininet} is a software commonly used in research, that can create a virtual network running real kernel, switch and application code with support for OpenFlow and Software-Defined Networking\cite{openflow}.

Figure~\ref{fig:topology} shows the topology used in our system: it is composed of ten routers, R1-R6 which I am going to refer to as \textit{outer routers}, and R7-R10 which I am going to call \textit{inner routers}. The topology includes also fourteen switches, the reasons of their presence is explained in section~\ref{sec:packet_counter}. For simplicity we assume that traffic is being generated only by the outer routers, therefore the inners' are only responsible of forwarding other nodes packets. The topology is a partially connected network with multiple paths connecting the same source-destination pair; note that having multiple paths is crucial for the path prediction system: in this way, during the training phase, it will learn several different ways to go from a source A to a destination B.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth, keepaspectratio]{img/network_arch}
\caption{Model topology}
\label{fig:topology}

\end{figure}
\subsubsection{Routing information}
The default behavior of Mininet is to create a layer 2 topology, with all the nodes acting as a switch or a host in a local network. Our objective is to build a path prediction system that learns from the routing algorithms, hence we need to build a fully functional level 3 network with level 3 routers. The easiest way to use Mininet as a layer 3 network is through MiniNext~\cite{mininext}, a mininet extension layer that support routing engines and PID namespaces. As a routing engine we use Quagga~\cite{quagga}, a routing suite providing implementation of routing protocols for Unix platforms. The Quagga architecture consists of a core daemon, zebra, that acts as an abstraction layer to the underlying Unix kernel and presents the Zserv API over a Unix or TCP stream to Quagga clients. It is these Zserv clients which typically implement a routing protocol and communicate routing updates to the zebra daemon. Mininext allows us to place routers into a mininet topology; the way it does this is by instantiating  a regular Mininet host node in a separate namespace and starting a routing process on it: different namespaces are necessary so that each host can run the routing process without interfering with the others. Figure~\ref{fig:quagga_mininext} shows an outline of this architecture: each Mininet host runs in a separate namespace and lunch the zebra and the routing daemons; the zebra daemons updates each router routing table with the routing informations exchanged through the virtual network built with the Linux Kernel.
\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{img/quagga_mininext}
\caption{Quagga router implementation in MiniNext}
\label{fig:quagga_mininext}
\end{figure}

\subsubsection{Router packet counter}
\label{sec:packet_counter}
The last elements necessary before putting everything together are the statistics about the number of packets on each router. In section~\ref{sec:net_topology} we have described the topology without giving an explanation of the presence of a switch between every pair of connected routers. The reason for the switches is that we want this system to work in a SDN context: the only way to use a SDN controller in Mininet is to use switches. There is also a secondary reason that justifies the switches presence: unlike the routers, switches do not run in a separate namespace, allowing traffic analyzer tools such as Wireshark to capture traffic on all the network interfaces at once. Because of the separate PID namespaces, if we wanted to capture traffic directly on the routers interfaces we would need a Wireshark instance for each router, that would make debugging extremely hard. 

A SDN controller can retrieve switches information with the OpenFlow protocol; in this project we use Ryu~\cite{ryu}, a SDN controller written in Python. Since the controller is connected to the switch, getting the packet count on the router is not trivial. On an OpenFlow enabled switch it is possible to get the number of packet received on each port; given that each router is directly connected to at least one switch, it is possible to infer the number of packets directed to the router by counting the number of packets coming from a certain port of the switches it is connected to. Figure~\ref{fig:packet_counter} explains this concept with an example: in the figure, the number of incoming packets on R1 at a given time is the sum of the incoming packets on port1 of S1 and port2 of S2. By repeating this procedure for all the routers in the network, the controller is capable of retrieving, at any given time, the number of incoming packets on each router.
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{img/packet_counter}
\caption{Router packet counter explanation}
\label{fig:packet_counter}
\end{figure}

% maybe swap dataset generation and deep learning model order
\subsubsection{Dataset Generation}
Up to this point we have a fully functional level 3 network, with a running instance of the OSPF routing protocol, connected to a controller capable of retrieving the packet count on each router when needed. To build our system we need a set of labeled data from which the model can learn. More specifically, we need a collection of samples containing at a given time, the packet counting information together with the routing information. The detail about the data format and how they are used are described in the following section~\ref{sec:dl_model}; in the rest of this paragraph we describe how we generate these data.

The idea is to simulate the traffic conditions in a regularly functioning network, collect the traffic and routing information, and use it to train the network. By default, a mininet network is static, in the sense that there is no traffic unless manually generated. One way to generate traffic is by using \textit{iperf}~\cite{iperf}, a tool for bandwidth measurement in IP networks.
As a second step, we need to change the output of OSPF over time to explore the various paths in the network. OSPF computes the path cost according to the interface speed, therefore it is sufficient to change the speed of the interfaces on the routers to make OSPF compute the new routes. 

To summarize, we generate the dataset as follows: for \textit{ospf\_configurations} times, every \textit{ospf\_conf\_duration} seconds, the network is teared down and rebuilt with new link speed, producing a new OSPF configuration; for each OSPF configuration, traffic is generated in the network by running \textit{iperf} between every source-destination pair, with a certain \textit{transmission\_probability} and a \textit{transmission\_time} duration. In the meantime, the controller retrieves the packets count every \textit{sampling\_time} seconds and saves it to a file, while another script saves the routing tables of each router, every time that the OSPF configuration changes. The pseudo-code of the algorithm is shown in figure~\ref{fig:traffic_gen} while table~\ref{tab:dataset_params} describes the algorithm parameters.

\begin{algorithm}
\captionof{figure}[Traffic generation algorithm]{Traffic generation algorithm}\label{fig:traffic_gen}
\begin{algorithmic}
\FORALL{\textit{ospf\_configurations}}
	\FORALL{\textit{(src, dst) pairs}}
		\STATE{$p=random(0, 1)$}
		\IF{$p<transmission\_probability$}
			\STATE{$t=randint(0, transmission\_time)$}
			\STATE{\textit{run iperf for t seconds}}
		\ENDIF
	\ENDFOR{}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{table}[h]
\resizebox{\textwidth}{!}{%
	\begin{tabular}{|l|c|l|ll}
	\cline{1-3}
	\multicolumn{1}{|c|}{\textbf{Parameter}} & \textbf{Value} & \multicolumn{1}{c|}{\textbf{Description}}                        &  &  \\ \cline{1-3}
	\textit{ospf\_configurations}            & 15             & number of different OSPF configurations                   &  &  \\ \cline{1-3}
	\textit{ospf\_conf\_duration}            & 20 m           & time after which a new ospf configuration is produced            &  &  \\ \cline{1-3}
	\textit{transmission\_probability}       & 0.65           & probability with which there is traffic between a pair of routers &  &  \\ \cline{1-3}
	\textit{transmission\_time}              & 0-5 s          & time in seconds to transmit for (iperf)                          &  &  \\ \cline{1-3}
	\textit{sampling\_time}                  & 1 s            & sampling time of the packet count                                &  &  \\ \cline{1-3}
	\end{tabular}%
	}
\caption{Dataset generation parameters}
\label{tab:dataset_params}
\end{table}
Using the parameters of table~\ref{tab:dataset_params}, we obtain a dataset of 17696 samples; the 85\% of these samples forms the training set, the remaining 15\% is test set. 

\subsection{Deep Learning Model}
\label{sec:dl_model}
The deep learning model chosen for this project is a LSTM recurrent neural network (RNN): recurrent neural network is a class of neural network in which connections between nodes form a directed cycle (\ref{fig:rnn}). These connections allow the nodes to memorize information about what has been computed so far. The reason we choose RNNs is because of their ability to make use of sequential information and to exhibit a dynamic temporal behavior. We want our model to learn the correlation between changes in the packet distribution and routing decisions over time.

Up to this moment we have talked about a model as if a single model would be able to  predict every path for every possible source-destination pair; however, given the complexity of routing, this scenario does not seem feasible. The solution is to train a separate model for every $(s,d)$, resulting in several simpler models rather than a single, very complex one. To give an idea of the order of magnitude of this approach, a network with $N$ nodes will result in $N(N-1)$ models. These numbers need to be adjusted if each node has, like in our case, more than one network interface; in this scenario there will be more model than the previous case, each one describing a particular $(src\_router, dst\_address)$ pair.
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{img/rnn}
\caption{Recurrent neural network and its unfolded version.\protect\footnotemark }
\label{fig:rnn}
\end{figure}
\footnotetext{http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/}

\subsubsection{Input/Output modeling}
A machine learning model requires a proper representation of the input and output; supervised learning involves a sample space $X$ and a label space $Y$, with the network responsible of learning a mapping function from values in $X$ to labels in $Y$, for each $(x_i, y_i) \in X\times Y$. Our input/output modeling follows the same approach described in~\cite{Kato}: given $O$ the set of outer routers, and $R$ the set of all the routers in the network, for each $(s, d) \in R \times O$, the system learns the next hop for that particular target destination. Note that the fact that inner routers are included in the set of sources nodes is not a conflict with what stated in~\ref{sec:net_topology}: even though these router do not generate traffic, they are still responsible of forwarding packets coming from other sources, making therefore necessary to model this behavior. 

Each model learns the next hop for a particular destination, given the packet count on each router at a given time $t$: the easiest way to model the input is an $N$\textit{-dimensional} array, with N being the number of routers in the network. The array is indexed by the router number, so the \textit{i-th} element of the array is the number of incoming packets on router \textit{i}. The output is modeled as a one-hot encoded, router indexed array, with a 1 in the position indexed by the predicted next hop; the size of the output is again equal to the number of routers in the network.

\subsubsection{Neural Network Architecture}
The architecture of a deep neural network is determined by the number of layers, the number of processing units (neurons) per layer and the interconnections between the layers. Choosing these parameters once at the beginning, hoping to achieve good performance, is not feasible given the impossibility to derive them from a formal description of the problem; thus, these parameters need to be tuned in a preliminary phase. As a general rule, a network too small will not be able to solve the problem and a network too big will probably overfit on the training set; there is also consensus on the fact that for the majority of the problems, adding additional hidden layers does not significantly improve the performance.

To define the parameters of our network, we follow these steps:
\begin{itemize}
\item pick a source-destination pair that requires a complex model
\item cross-validate each combination of layer and neurons
\item choose the combination with the best combination of accuracy and loss.
\end{itemize}

It is important to clarify what we mean with \textit{"source-destination pair that requires a complex model"}: cross-validating the different architectures on all the possible pairs would be excessively time-consuming, thus we decide to perform the validation on a single target -- from now on, a target is simply a source-destination pair. For this validation to be meaningful, we need our target to be representative enough of the problem we are modeling: since we want our system to learn alternative paths, it would not make sense to choose a target of two directly connected nodes, because the resulting model would be too simple. As a consequence of these considerations, we choose the target $(R1, R4)$; figure~\ref{fig:validation_target} illustrates some of the 3-hops paths connecting the two routers.
\begin{figure}[]
\centering
\includegraphics[width=0.85\textwidth]{img/validation_target}
\caption{Examples of paths connecting the validation target.}
\label{fig:validation_target}
\end{figure}
As it is noticeable from the figure, there are several paths with the same number of hops connecting the two routers, so, in the context of choosing a representative model, the selected target seems a good candidate.

In the cross-validation phase, we test 24 different configurations by trying all the combinations of the following parameters:
\begin{itemize}
\item $hidden\_layers=\{2,4,6,8\}$
\item $neurons=\{4, 8,16,32,64,128\}$.
\end{itemize}
It is important to remind that each hidden layer is a recurrent layer with LSTM cell.
Each configuration is tested 10 times on different partitions of the dataset, producing the results in table~\ref{tab:arch_results}. The table shows two metrics: accuracy (percentage of samples correctly classified) and cross-entropy loss (distance between predicted and true label distribution); it is evident that what we described earlier about how adding layers does not significantly improve the performance, applies in this case. All the noticeable improvements in the results are caused by increasing the number of neurons in each layer (as you may notice by reading the table line by line); instead, if you read the table column wise (fixed number of neurons and increasing number of layers), you will not notice any performance gain. A 4-layers 128-neurons achieves the best performance in terms of accuracy whereas a 4-layers 128-neurons has the lowest loss; it is worth noticing that overall, given a fixed number of neurons, the performances typically differ for less than $1\%$ in accuracy. % Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{1}{|l|}{\textbf{}} & \multicolumn{12}{c|}{\textbf{Neurons}} \\ \hline
\multicolumn{1}{|l|}{\textbf{Layers}} & \multicolumn{2}{c|}{4} & \multicolumn{2}{c|}{8} & \multicolumn{2}{c|}{16} & \multicolumn{2}{c|}{32} & \multicolumn{2}{c|}{64} & \multicolumn{2}{c|}{128} \\ \hline
\multicolumn{1}{|l|}{} & \textit{Accuracy \%} & \textit{Loss} & \textit{Accuracy \%} & \textit{Loss} & \textit{Accuracy \%} & \textit{Loss} & \textit{Accuracy \%} & \textit{Loss} & \textit{Accuracy \%} & \textit{Loss} & \textit{Accuracy \%} & \textit{Loss} \\ \hline
2 & 86.59\% & 0.5147 & 88.82\% & 0.3946 & 92.95\% & 0.3026 & 94.85\% & 0.2159 & 95.60\% & 0.1659 & 96.08\% & 0.1368 \\ \hline
4 & 76.48\% & 0.6416 & 87.69\% & 0.4644 & 92.99\% & 0.2435 & 94.90\% & 0.2045 & 95.70\% & 0.1554 & \textbf{96.58\%} & 0.1214 \\ \hline
6 & 64.90\% & 0.7727 & 88.82\% & 0.4450 & 92.72\% & 0.2515 & 95.25\% & 0.1663 & 95.38\% & 0.1541 & 96.14\% & \textbf{0.1149} \\ \hline
8 & 65.84\% & 0.7953 & 87.42\% & 0.4914 & 91.01\% & 0.3340 & 95.08\% & 0.1718 & 95.83\% & 0.1374 & 95.73\% & 0.1361 \\ \hline
\end{tabular}%
}
\caption{LSTM architectures comparison}
\label{tab:arch_results}
\end{table}


From this first analysis it is clear that to achieve the best results, each layer of the network needs to have 128 processing units; however deciding on the number of layers is not that immediate. As previously discussed, the gain in performance with an increase of the number of layers is not noticeable, therefore we decide to take into account further factors to choose the final architecture. Table~\ref{tab:arch_timing} shows the average training time for the different configurations: in this case the impact of additional layers is evident. The training time seems to vary linearly with the number of layers: if we double the layers we basically double the training time. Considering the limited computational power and time, and the number of models we need to train, we decide to use a network with two layers, as a good compromise between performance and time.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{6}{c|}{\textbf{Neurons}} \\ \hline
\textbf{Layers} & 4 & 8 & 16 & 32 & 64 & 128 \\ \hline
2 & 301.20 s & 304.82 s & 314.56 s & 326.72 s & 388.46 s & 623.84 s \\ \hline
4 & 487.49 s & 499.91 s & 516.06 s & 557.60 s & 675.94 s & 1129.96 s \\ \hline
6 & 686.25 s & 705.11 s & 722.09  s & 781.28 s & 961.83 s & 1649.99 s \\ \hline
8 & 905.18 s & 931.16 s & 972.52 s & 1029.73 s & 1275.34 s & 2121.93 s \\ \hline
\end{tabular}
\caption{LSTM architectures average training time}
\label{tab:arch_timing}
\end{table}

As a result of this analysis, the final architecture is composed as follows:
\begin{itemize}
\item input layer (10 neurons)
\item two hidden layers (128 neurons ea., $tanh$ activation)
\item output layer (10 neurons, sigmoid activation)
\end{itemize}

To increase training efficiency and avoid overfitting, we apply a batch normalization layer on the input, dropout on both the recurrent and feedforward connections, and $L2$ regularization to the loss function. Figure~\ref{fig:normalization_cmp} shows the impact of normalization on the training performance: for both accuracy and loss, the normalization layer dramatically improves the performance, with a gain of about the 20\%.

\begin{figure}[]
\makebox[\textwidth]{\centering
\includegraphics[width=0.88\paperwidth]{img/normalization_cmp}}
\caption{Impact of input normalization on training.}
\label{fig:normalization_cmp}
\end{figure}

\section{Implementation}
\label{sec:implementation}
To prototype our system we use the following tools:
\begin{itemize}
\item Ryu: SDN controller written in Python~\cite{ryu}
\item Google Protocol Buffer: framework for serializing structured data~\cite{protobuf}
\item Mininet: virtual network creation tool~\cite{mininet}
\item Quagga: routing software suite~\cite{quagga}
\item Keras: high-level neural networks API written in Python~\cite{keras}
\item Tensorflow: as Keras backend for the deep learning implementation~\cite{tensorflow}
\item Python: as a main programming language.
\end{itemize}