\chapter{Background}
\label{ch:background}
Before describing our approach, we would like to give a brief introduction to some of the methods and concepts that are needed to understand our implementation and related work. Specifically, we first shortly describe the network technologies required to understand the aim of this project; next, we give an introduction about machine learning and its applications. Even though these might seem separate topics,  keep in mind that in this project, we take advantage of machine learning to address networking problems.

\section{Software-Defined Networking}
Software-Defined Networking (SDN) is paradigm that promises to break networks' vertical integration of control and data plan, separating the network's control logic from the underlying routers and switches, promoting (logical)  centralization  of  network  control,  and  introducing  the ability  to  program  the  network.  The  separation  of  concerns introduced   between   the   definition   of   network   policies,   their implementation  in  switching  hardware,  and  the  forwarding  of traffic, is key to the desired flexibility: by breaking the network control  problem  into  tractable  pieces,  SDN  makes  it  easier  to create and introduce new abstractions in networking, simplifying network  management  and  facilitating  network  evolution~\cite{kreutz2015software}.SDN is a network architecture built on four principles:
\begin{enumerate}
\item separate control and data planes: network devices are left responsible only of packet forwarding
\item flow-based forwarding: packets are forwarded by looking to a set of fields instead of only the destination, introducing great flexibility~\cite{mckeown2008openflow}
\item external control logic: the control logic is moved to an external entity called SDN controller or Network Operating System, that is a software platform that provides abstractions to facilitate the programming of forwarding devices
\item programmable network: software running on top of the NOS allows to program the network; this is considered the main value of SDN.
\end{enumerate}
SDN has successfully opened the way towards a next generation networking, creating an innovative research
and  development  environment,  promoting  advances  in  switch  and  controller platform design,  evolution of performance  of  devices  and  architectures, security and dependability.

\section{Knowledge-Defined Networking}
Knowledge-defined networking (KDN)~\cite{mestres2017knowledge} is a new paradigm that promotes the application of Artificial Intelligence (AI) to control and operate networks thanks to the rise of SDN. SDN provides a centralize control plane, a logical single point with knowledge of the network; moreover, current network devices have improved computing capabilities, which allow them to perform monitoring operations commonly referred to as network telemetry~\cite{kim2015band}. Information provided by network telemetry are usually provided to a centralized Network Analytics (NA) platform~\cite{clemm2015dna}, that combined with SDN can bring to light the Knowledge Plane proposed in~\cite{clark2003knowledge}. Knowledge-Defined Networking is the paradigm resulting from the combination of these tools, specifically Software Defined Networking, Network Analytics and Knowledge Plan. In the KDN paradigm, the knowledge plane has a rich view of the network; this view is transformed into knowledge via Machine Learning (ML) and used to make decisions. The ultimate goal of KDN is to combined SDN, NA and ML to provide automated network control.
%
%\section{Routing}
In networking, routing is the process of selecting a path in or between networks. Routing directs network packets from their source toward their destination through intermediate network nodes by specific packet forwarding mechanisms. Forwarding is performed on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, is very important for efficient routing.Dynamic routing constructs routing tables automatically thanks to the information carried by the routing protocols, that are usually classified in \textit{distance vector} and \textit{link-state} algorithms.

%\paragraph{BGP} (Border Gateway Protocol) is a protocol designed to exchange routing and reachability information among or within autonomous systems. In this second application it is also referred to as Internal BGP, or iBGP. In contrast, the Internet application of the protocol may be referred to as External BGP, or eBGP. BGP is typically used by Internet Service Providers (ISP) to establish routing between one another, or in large private networks to join a number of large Open Shortest Path First (OSPF) networks, when OSPF by itself does not scale to the size required. Routing informations are exchanged between neighbors called peers; in order to communicate, these peers require a manual configuration.

\paragraph{OSPF} is a link-state routing algorithm for IP networks. OSPF falls in the family of iBGP and it is widely adopted in large networks. It works thanks to a map of the network, built by gathering link state information from available routers. The maps is used to compute the shortest-path tree for each route using a method based on Dijkstra's algorithm. The OSPF routing policies are dictated by link metrics associated with each routing interface, typically the interface speed.

%\section{Machine Learning}
Machine learning is a field of computer science that studies the ability of making computers learn without explicitly programming them~\cite{5392560}. In machine learning there are three main approaches: supervised learning, unsupervised learning and reinforcement learning. Supervised learning is used to classify labeled data, the label is a sort of supervisor describing the class of an observation. In unsupervised learning on the other hand there is no supervisor, meaning that data are unlabeled and the goal is to find the hidden relation of the data. Reinforcement learning is learning what actions to do so as to maximize a numerical reward signal without being told which actions to take but instead discovering which yield the most reward by trying them \cite{Sutton98reinforcementlearning}. Among the several branches of machine learning, neural networks and in particular deep learning, have recently attracted a lot of attentions.

%\subsection{Deep Learning}
Teaching a computer to solve tasks that are hard to describe formally (e.g. speech recognition) is challenging; the solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning~\cite{Goodfellow-et-al-2016}. In other words, deep learning can be described as the set of machine learning techniques that concatenate multiple layers of processing units (typically non-linear), for feature extraction and transformation  ~\cite{deep_learning_heterogeneus}. The first working deep learning algorithm was a multilayer perceptrons developed by Ivakhnenko et al.~\cite{ivakhnenko1973cybernetic}. Later on new techniques including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) were developed; these techniques dramatically increased AI performance in tasks such as image recognition. More recently deep learning has been used to build AlphaGo~\cite{alphago}, a program that plays the board game Go and managed to beat, in 2017, the Go world No.1 Ke Jie.

\section{LSTM}
Long Short-Term Memory cells are a further development of Recurrent Neural Networks (RNNs). They were developed by Hochreiter and Schmidhuber in 1997 and have been further improved ever since~\cite{Greff2016}. The major flaw of a traditional neural network is that it does not capture the relation between the input it currently looks at and the previous training example. This however, is crucial for e.g. developing a language model. Making predictions about a subsequent word strongly depends on the preceding words. RNNs try to solve this problem; yet it turns out that they are not scalable to model long-term dependencies in practice. This is due to numerical problems commonly referred to as the \textit{vanishing/exploding gradient} when weight updates are backpropagated through the time steps. LSTMs overcome this problem and enable capturing long-term temporal dependencies among the input elements. They are considered state-of-the-art in numerous sequential prediction tasks such as Speech Recognition, Handwriting Recognition, Language Translation and many others.

The novelty of LSTMs compared to conventional RNNs is the introduction of the \textit{LSTM cell}. Figure \ref{fig:lstm} gives an overview over such a cell that is repeated three times, each receiving the current input as well as the output of the previous cell. For instance, the prediction $h_{t+1}$ (through feedforward) is based on the corresponding input $x_{t+1}$ as well as on the output of the previous cell. The LSTM cell is responsible for maintaining and updating a state that keeps track of the input that has been processed over time. Which information precisely should be kept and which overwritten is decided based on the training of the network. Each cell has associated weights that are updated during each backward pass such that the cell keeps the information that optimizes predictions. The major advantage of introducing this cell is that the \textit{Backpropagation Through Time} does not need to flow through numerous activation gates between the hidden layers. The transfer from cell to cell only flows through pointwise multiplications and additions. This way the numerical problems of RNNs are avoided.

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{img/lstm}
\caption{LSTM cell overview.\protect\footnotemark}
\label{fig:lstm}
\end{figure}
\footnotetext{Taken from colah's blog: http://colah.github.io/posts/2015-08-Understanding-LSTMs/}

